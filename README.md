# Smart-Cities-

This project is aimed to . (co-authored by Dimitris Samakovlis and Johana Tirana)

First of all, all files mentioned below, except the report of course, must be placed in the same folder for the notebooks to properly work. In the GitHub repository they are separated in different folders only for the purposes of clarification and order.

This GitHub repository contains all the Jupyter Notebooks and any additional necessary files for this project to work. More specifically it contains:

1) a folder named "Notebooks" containing the "ACCIDENTS", "Air Quality", "Metro", "Smart_Electricity" notebooks

2) a folder named "Datasets" containing all the datasets used in the notebooks

3) a folder named "Report" containing the report of this project

Here is a quick summary of the notebooks:

1) ACCIDENTS: 
 
2) Air Quality: 

3) Metro: 
 
4) Smart_Electricity: 

And here is a quick summary of the datasets:

1) caracteristics: 

2) holidays: 

3) places: 

4) users:

5) vehicles:

6) AirQualityUCI:

7) Metro_Interstate_Traffic_Volume:

8) weather_daily_darksky:

9) uk_bank_holidays:

10) informations_households:

11) acorn_details:

12) daily_dataset:

The most time consuming parts are the training of the algorithms or at least some of them, such as SVM. The same applies when the user wants to download new tweets. It is a slow and steady process and noone should expect thousands of tweets in mere minutes. Every notebook should be left to execute all cells and when it fully finishes to proceed to the next one, because "Sentiment Prediction" needs some information taken from "Sentiment Analysis" when it fully finishes the execution and "Fake News Detection" needs the same from "Sentiment Prediction", which means that not all three can run at the same time even if someone had a computer to do such a heavy task.

The whole process for my computer lasted around 10 hours for me. My hardware consists of 8 gigabytes of ddr3 ram and the first generation of i7 processor. They are undoubtedly very old components that are operating really slow and can easily terminate the execution when they reach their limits. In a newer and faster computer with ddr4 ram of 8 or more gigabytes and a faster newer processor this project should be executed much faster and give the opportunity to the user to use more data in every process, such as more tweets to predict, more input data for the classification algorithms and different configurations for the algorithms that transform the texts into numerical arrays.  


Feel free to experiment with any algorithm, any process or even use totally different datasets from what I used!
